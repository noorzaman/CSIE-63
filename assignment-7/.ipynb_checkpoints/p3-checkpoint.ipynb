{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['sinh', 'trunc', 'tan', 'cosh', 'rank', 'radians', 'sin', 'mean', 'log2', 'expm1', 'rand', 'rint', 'array', 'size', 'broadcast', 'exp', 'sum', 'randn', 'ceil', 'isnan', 'repeat', 'cos', 'degrees', 'tanh', 'sqrt', 'split', 'cbrt', 'hypot', 'log', 'log10', 'log1p', 'floor']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark.mllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "get_ipython().magic(u'pylab inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Initialize streaming context\n",
    "conf = SparkConf()\\\n",
    "                .setMaster(\"local[2]\")\\\n",
    "                .setAppName(\"AutoRegression\")\\\n",
    "                .set(\"spark.executor.memory\", \"2g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName(\"spark play\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header:\n",
      "['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\n",
      "\n",
      "Training Data column length: 13\n",
      "[('1', '0', '1', '0', '0', '6', '0', '1', '0.24', '0.2879', '0.81', '0', '16'), ('1', '0', '1', '1', '0', '6', '0', '1', '0.22', '0.2727', '0.8', '0', '40'), ('1', '0', '1', '2', '0', '6', '0', '1', '0.22', '0.2727', '0.8', '0', '32'), ('1', '0', '1', '3', '0', '6', '0', '1', '0.24', '0.2879', '0.75', '0', '13'), ('1', '0', '1', '4', '0', '6', '0', '1', '0.24', '0.2879', '0.75', '0', '1')]\n"
     ]
    }
   ],
   "source": [
    "# ## Extract the variables we want to keep\n",
    "# \n",
    "# _All variables:_\n",
    "# \n",
    "# `instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt`\n",
    "# \n",
    "# _Variables to keep:_\n",
    "# \n",
    "# `season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,cnt`\n",
    "\n",
    "path = \"file:////Users/swaite/Stirling/CSIE-63/assignment-7/data/bike-sharing-hour.csv\"\n",
    "bike_rdd = sc.textFile(path, use_unicode=False) \\\n",
    "                    .map(lambda x: x.split(\",\"))\n",
    "header = bike_rdd.first()\n",
    "print \"header:\"\n",
    "print(header)\n",
    "# x[2]-x[13], x[16]\n",
    "bike_rdd_train = bike_rdd.filter(lambda x: x != header)\\\n",
    "                         #.map(lambda x: (x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[12],x[13],x[16]))\\\n",
    "                         .cache()\n",
    "print(\"\\nTraining Data column length: \" + str(len(bike_rdd_train.first())))\n",
    "print(bike_rdd_train.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature mappings for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get the categorical feature mapping for a given variable column\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of second categorical feature column: {'11': 0, '10': 6, '12': 7, '1': 1, '3': 2, '2': 8, '5': 3, '4': 9, '7': 4, '6': 10, '9': 5, '8': 11}\n"
     ]
    }
   ],
   "source": [
    "# we want to extract the feature mappings for columns 2 - 9\n",
    "# try it out on column 2 first\n",
    "print \"Mapping of second categorical feature column: %s\" % get_mapping(bike_rdd_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 1154\n",
      "Feature vector length for numerical features: 13\n",
      "Total feature vector length: 1167\n"
     ]
    }
   ],
   "source": [
    "# extract all the categorical mappings\n",
    "mappings = [get_mapping(bike_rdd_train, i) for i in range(2,13)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(bike_rdd_train.first())\n",
    "total_len = num_len + cat_len\n",
    "print \"Feature vector length for categorical features: %d\" % cat_len \n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to use the feature mappings to extract binary feature vectors, and concatenate with\n",
    "# the numerical feature vectors\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[2:9]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    \n",
    "    num_vec = np.array([float(field) for field in record[10:14]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "\n",
    "# function to extract the label from the last column\n",
    "def extract_label(record):\n",
    "    return float(record[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: <function first at 0x10ab63de8>\n",
      "Label: 16.0\n",
      "Linear Model feature vector:\n",
      "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,16.0]\n",
      "Linear Model feature vector length: 1157\n"
     ]
    }
   ],
   "source": [
    "data = bike_rdd_train.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))\n",
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first)\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree feature vector: [1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0,16.0]\n",
      "Decision Tree feature vector length: 11\n"
     ]
    }
   ],
   "source": [
    "# we need a separate set of feature vectors for the decision tree\n",
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[2:14]))\n",
    "    \n",
    "data_dt = bike_rdd_train.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "first_point_dt = data_dt.first()\n",
    "print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Regression Model on the Bike Sharing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(16.0, -1.9894012556799439e+36), (40.0, -4.9721324143362443e+36), (32.0, -3.9778862866476995e+36), (13.0, -1.6165443882305017e+36), (1.0, -1.2517602536659713e+35)]\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\n",
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree predictions: [(16.0, 17.01814882032668), (40.0, 44.286852589641434), (32.0, 30.96880733944954), (13.0, 17.01814882032668), (1.0, 5.6638457921955805)]\n",
      "Decision Tree depth: 5\n",
      "Decision Tree number of nodes: 63\n"
     ]
    }
   ],
   "source": [
    "# we pass in an empty mapping for categorical feature size {}\n",
    "dt_model = DecisionTree.trainRegressor(data_dt, {})\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "actual = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perfomance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up performance metrics functions \n",
    "\n",
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2\n",
    "\n",
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)\n",
    "\n",
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model - Mean Squared Error: 1062643507737328563794480371722602217184329868125552687618838621785224642560.0000\n",
      "Linear Model - Mean Absolute Error: 23547538462393464589505485305406291968.0000\n",
      "Linear Model - Root Mean Squared Log Error: nan\n"
     ]
    }
   ],
   "source": [
    "# compute performance metrics for linear model\n",
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
