{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch Iris Dataset from https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "# Make attached Python script, softmax_irises.py work. \n",
    "# You might have to upgrade the script to TF 1.x API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/install/migration\n",
    "# https://github.com/tensorflow/tensorflow/issues/11217\n",
    "# python2.7 tf_upgrade.py --infile softmax_irises-1.py  --outfile softmax_irises-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorboard --logdir 'problem-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax example in TF using the classical Iris dataset\n",
    "# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights form a matrix, not a column vector, one \"weight vector\" per class.\n",
    "W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n",
    "# so do the biases, one per class.\n",
    "b = tf.Variable(tf.zeros([3], name=\"bias\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_inputs(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(X):\n",
    "    return tf.nn.softmax(combine_inputs(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(X, Y):\n",
    "    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=combine_inputs(X), labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    # filename_queue = tf.train.string_input_producer([os.path.dirname(__file__) + \"/\" + file_name])\n",
    "    filename_queue = tf.train.string_input_producer([file_name])\n",
    "    print file_name\n",
    "    print filename_queue\n",
    "\n",
    "\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    print reader\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "    # decode_csv will convert a Tensor from type string (the text line) in\n",
    "    # a tuple of tensor columns with the specified defaults, which also\n",
    "    # sets the data type for each column\n",
    "    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\n",
    "    return tf.train.shuffle_batch(decoded,\n",
    "                                  batch_size=batch_size,\n",
    "                                  capacity=batch_size * 50,\n",
    "                                  min_after_dequeue=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs():\n",
    "    sepal_length, sepal_width, petal_length, petal_width, label =\\\n",
    "        read_csv(100, \"iris.data\", [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "\n",
    "    # convert class names to a 0 based class index.\n",
    "    label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n",
    "        tf.equal(label, [\"Iris-setosa\"]),\n",
    "        tf.equal(label, [\"Iris-versicolor\"]),\n",
    "        tf.equal(label, [\"Iris-virginica\"])\n",
    "    ])), 0))\n",
    "\n",
    "    # Pack all the features that we care about in a single matrix;\n",
    "    # We then transpose to have a matrix with one example per row and one feature per column.\n",
    "    features = tf.transpose(tf.stack([sepal_length, sepal_width, petal_length, petal_width]))\n",
    "\n",
    "    return features, label_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(total_loss):\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, X, Y):\n",
    "\n",
    "    predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n",
    "\n",
    "    print sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.data\n",
      "<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x11a8d7310>\n",
      "<tensorflow.python.ops.io_ops.TextLineReader object at 0x11a898590>\n",
      "('Step 0, loss: ', [1.0898832])\n",
      "\n",
      "\n",
      "('Step 10, loss: ', [1.0148832])\n",
      "\n",
      "\n",
      "('Step 20, loss: ', [0.99335462])\n",
      "\n",
      "\n",
      "('Step 30, loss: ', [0.91635209])\n",
      "\n",
      "\n",
      "('Step 40, loss: ', [0.86754453])\n",
      "\n",
      "\n",
      "('Step 50, loss: ', [0.8247748])\n",
      "\n",
      "\n",
      "('Step 60, loss: ', [0.83856642])\n",
      "\n",
      "\n",
      "('Step 70, loss: ', [0.79748917])\n",
      "\n",
      "\n",
      "('Step 80, loss: ', [0.79458284])\n",
      "\n",
      "\n",
      "('Step 90, loss: ', [0.78015739])\n",
      "\n",
      "\n",
      "('Step 100, loss: ', [0.71979094])\n",
      "\n",
      "\n",
      "('Step 110, loss: ', [0.70995116])\n",
      "\n",
      "\n",
      "('Step 120, loss: ', [0.69566315])\n",
      "\n",
      "\n",
      "('Step 130, loss: ', [0.65042061])\n",
      "\n",
      "\n",
      "('Step 140, loss: ', [0.6102156])\n",
      "\n",
      "\n",
      "('Step 150, loss: ', [0.59877431])\n",
      "\n",
      "\n",
      "('Step 160, loss: ', [0.58082783])\n",
      "\n",
      "\n",
      "('Step 170, loss: ', [0.62390381])\n",
      "\n",
      "\n",
      "('Step 180, loss: ', [0.58097643])\n",
      "\n",
      "\n",
      "('Step 190, loss: ', [0.58120489])\n",
      "\n",
      "\n",
      "('Step 200, loss: ', [0.56746191])\n",
      "\n",
      "\n",
      "('Step 210, loss: ', [0.55816239])\n",
      "\n",
      "\n",
      "('Step 220, loss: ', [0.56133652])\n",
      "\n",
      "\n",
      "('Step 230, loss: ', [0.53153962])\n",
      "\n",
      "\n",
      "('Step 240, loss: ', [0.57969779])\n",
      "\n",
      "\n",
      "('Step 250, loss: ', [0.58183497])\n",
      "\n",
      "\n",
      "('Step 260, loss: ', [0.5585283])\n",
      "\n",
      "\n",
      "('Step 270, loss: ', [0.55223984])\n",
      "\n",
      "\n",
      "('Step 280, loss: ', [0.57976758])\n",
      "\n",
      "\n",
      "('Step 290, loss: ', [0.56216758])\n",
      "\n",
      "\n",
      "('Step 300, loss: ', [0.55733645])\n",
      "\n",
      "\n",
      "('Step 310, loss: ', [0.52043241])\n",
      "\n",
      "\n",
      "('Step 320, loss: ', [0.51336884])\n",
      "\n",
      "\n",
      "('Step 330, loss: ', [0.48979232])\n",
      "\n",
      "\n",
      "('Step 340, loss: ', [0.4955796])\n",
      "\n",
      "\n",
      "('Step 350, loss: ', [0.45573741])\n",
      "\n",
      "\n",
      "('Step 360, loss: ', [0.4849405])\n",
      "\n",
      "\n",
      "('Step 370, loss: ', [0.47395021])\n",
      "\n",
      "\n",
      "('Step 380, loss: ', [0.46484834])\n",
      "\n",
      "\n",
      "('Step 390, loss: ', [0.45294175])\n",
      "\n",
      "\n",
      "('Step 400, loss: ', [0.50502634])\n",
      "\n",
      "\n",
      "('Step 410, loss: ', [0.45629418])\n",
      "\n",
      "\n",
      "('Step 420, loss: ', [0.46410194])\n",
      "\n",
      "\n",
      "('Step 430, loss: ', [0.47061715])\n",
      "\n",
      "\n",
      "('Step 440, loss: ', [0.48275575])\n",
      "\n",
      "\n",
      "('Step 450, loss: ', [0.47816962])\n",
      "\n",
      "\n",
      "('Step 460, loss: ', [0.4813253])\n",
      "\n",
      "\n",
      "('Step 470, loss: ', [0.46614602])\n",
      "\n",
      "\n",
      "('Step 480, loss: ', [0.46935669])\n",
      "\n",
      "\n",
      "('Step 490, loss: ', [0.4698095])\n",
      "\n",
      "\n",
      "('Step 500, loss: ', [0.44694763])\n",
      "\n",
      "\n",
      "('Step 510, loss: ', [0.43293726])\n",
      "\n",
      "\n",
      "('Step 520, loss: ', [0.39920396])\n",
      "\n",
      "\n",
      "('Step 530, loss: ', [0.40757698])\n",
      "\n",
      "\n",
      "('Step 540, loss: ', [0.4199791])\n",
      "\n",
      "\n",
      "('Step 550, loss: ', [0.40856537])\n",
      "\n",
      "\n",
      "('Step 560, loss: ', [0.37551537])\n",
      "\n",
      "\n",
      "('Step 570, loss: ', [0.43177658])\n",
      "\n",
      "\n",
      "('Step 580, loss: ', [0.38498887])\n",
      "\n",
      "\n",
      "('Step 590, loss: ', [0.41629776])\n",
      "\n",
      "\n",
      "('Step 600, loss: ', [0.4369638])\n",
      "\n",
      "\n",
      "('Step 610, loss: ', [0.4674888])\n",
      "\n",
      "\n",
      "('Step 620, loss: ', [0.43468148])\n",
      "\n",
      "\n",
      "('Step 630, loss: ', [0.42134598])\n",
      "\n",
      "\n",
      "('Step 640, loss: ', [0.42906526])\n",
      "\n",
      "\n",
      "('Step 650, loss: ', [0.43231899])\n",
      "\n",
      "\n",
      "('Step 660, loss: ', [0.44639343])\n",
      "\n",
      "\n",
      "('Step 670, loss: ', [0.38422185])\n",
      "\n",
      "\n",
      "('Step 680, loss: ', [0.41376564])\n",
      "\n",
      "\n",
      "('Step 690, loss: ', [0.37917018])\n",
      "\n",
      "\n",
      "('Step 700, loss: ', [0.39813682])\n",
      "\n",
      "\n",
      "('Step 710, loss: ', [0.39023185])\n",
      "\n",
      "\n",
      "('Step 720, loss: ', [0.3781763])\n",
      "\n",
      "\n",
      "('Step 730, loss: ', [0.36176285])\n",
      "\n",
      "\n",
      "('Step 740, loss: ', [0.36281639])\n",
      "\n",
      "\n",
      "('Step 750, loss: ', [0.38528335])\n",
      "\n",
      "\n",
      "('Step 760, loss: ', [0.39869568])\n",
      "\n",
      "\n",
      "('Step 770, loss: ', [0.42683715])\n",
      "\n",
      "\n",
      "('Step 780, loss: ', [0.40651625])\n",
      "\n",
      "\n",
      "('Step 790, loss: ', [0.40472299])\n",
      "\n",
      "\n",
      "('Step 800, loss: ', [0.36900687])\n",
      "\n",
      "\n",
      "('Step 810, loss: ', [0.40956092])\n",
      "\n",
      "\n",
      "('Step 820, loss: ', [0.40475693])\n",
      "\n",
      "\n",
      "('Step 830, loss: ', [0.40065658])\n",
      "\n",
      "\n",
      "('Step 840, loss: ', [0.39783397])\n",
      "\n",
      "\n",
      "('Step 850, loss: ', [0.38222122])\n",
      "\n",
      "\n",
      "('Step 860, loss: ', [0.37465438])\n",
      "\n",
      "\n",
      "('Step 870, loss: ', [0.39134988])\n",
      "\n",
      "\n",
      "('Step 880, loss: ', [0.37413925])\n",
      "\n",
      "\n",
      "('Step 890, loss: ', [0.36541235])\n",
      "\n",
      "\n",
      "('Step 900, loss: ', [0.37903121])\n",
      "\n",
      "\n",
      "('Step 910, loss: ', [0.35455373])\n",
      "\n",
      "\n",
      "('Step 920, loss: ', [0.36283669])\n",
      "\n",
      "\n",
      "('Step 930, loss: ', [0.37397468])\n",
      "\n",
      "\n",
      "('Step 940, loss: ', [0.35933772])\n",
      "\n",
      "\n",
      "('Step 950, loss: ', [0.35727423])\n",
      "\n",
      "\n",
      "('Step 960, loss: ', [0.33908793])\n",
      "\n",
      "\n",
      "('Step 970, loss: ', [0.36061683])\n",
      "\n",
      "\n",
      "('Step 980, loss: ', [0.366622])\n",
      "\n",
      "\n",
      "('Step 990, loss: ', [0.35954541])\n",
      "\n",
      "\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session, setup boilerplate\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    X, Y = inputs()\n",
    "\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"problem-3\", graph=tf.get_default_graph())\n",
    "    tf.summary.scalar('loss1', total_loss)\n",
    "\n",
    "    # actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 10 == 0:\n",
    "            writer.add_summary(sess.run(tf.summary.merge_all()), global_step=step)\n",
    "            print(\"Step {}, loss: \".format(step), sess.run([total_loss]))\n",
    "\n",
    "    evaluate(sess, X, Y)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
