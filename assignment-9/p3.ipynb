{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "from kafka.errors import OffsetOutOfRangeError\n",
    "from kafka.errors import KafkaError\n",
    "# from pyspark import SparkConf\n",
    "# from pyspark import SparkContext\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# from pyspark.streaming.kafka import KafkaUtils\n",
    "from itertools import islice\n",
    "# import logging\n",
    "# import sys\n",
    "import time\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import csv\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rather than using splitAndSend.sh bash script to generate traffic towards Spark Streaming engine, \n",
    "# write a Kafka Producer which will read orders.txt file \n",
    "# and send 1,000 orders to a Kafka topic every second. \n",
    "\n",
    "# orders.txt example output\n",
    "# 2016-03-22 20:25:28,12,92,WLL,701,67.00,B\n",
    "# 2016-03-22 20:25:28,13,92,SRPT,593,23.00,B\n",
    "# 2016-03-22 20:25:28,14,62,FB,975,100.00,B\n",
    "# 2016-03-22 20:25:28,15,100,ABX,186,29.00,S\n",
    "# 2016-03-22 20:25:28,16,48,YHOO,206,89.00,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kafka_consumer():        \n",
    "#     topic = \"p3-topic\"\n",
    "#     group = \"p3-group-1\"\n",
    "#     bootstrap_servers = ['localhost:9092']\n",
    "# #     bootstrap_servers = [broker_str]\n",
    "#     print('Topic is: ', topic)\n",
    "#     print('Group is: ', group)\n",
    "#     consumer = KafkaConsumer(group_id=group, bootstrap_servers=bootstrap_servers, auto_offset_reset=\"latest\")\n",
    "#     # To consume latest messages and auto-commit offsets\n",
    "#     try:\n",
    "#         consumer.subscribe([topic])\n",
    "#         while True:\n",
    "#             # Process messages\n",
    "#             try:\n",
    "#                 k_msg = consumer.poll(timeout_ms=200)\n",
    "#             except OffsetOutOfRangeError:log.info(\"Offset out of range. Seeking to begining\")\n",
    "#                 # consumer.seek_to_beginning(tp)\n",
    "#                 # You can save `consumer.position(tp)` to redis after this,\n",
    "#                 # but it will be saved after next message anyway\n",
    "#             else:\n",
    "#                 if k_msg:\n",
    "#                     for msgs in list(k_msg.values()):\n",
    "#                         for msg in msgs:\n",
    "#                             print('new msg: ', str(msg))\n",
    "#                             # Process message and increment offset\n",
    "#                             print('partition: ', msg.partition, 'message offset: ', msg.offset)\n",
    "#     except KafkaError as e:\n",
    "#         log.info('Got kafka error %s: %s' % (str(e), type(e)))\n",
    "#     except Exception as e:\n",
    "#         log.info('Got exception %s: %s' % (str(e), type(e)))\n",
    "#     else:\n",
    "#         log.info('No exception raised!')\n",
    "#     finally:\n",
    "#         consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "def kafka_producer():\n",
    "    infile_path = \"/Users/swaite/Stirling/CSIE-63/assignment-9/data/orders.txt\"\n",
    "    chunk_size = 1000\n",
    "    topic = \"p3-topic\"\n",
    "    producer = KafkaProducer(bootstrap_servers=\"localhost:9092\")\n",
    "    \n",
    "    with open(infile_path) as f:\n",
    "        while True:\n",
    "            chunks = list(islice(f, chunk_size))\n",
    "            print(len(chunks))\n",
    "            chunked_message = \"\".join(chunks)\n",
    "            producer.send(topic, chunked_message)\n",
    "            if not chunks:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "    \n",
    "kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}